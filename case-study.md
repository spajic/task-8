# Case-study оптимизации

## Актуальная проблема
Суть проблемы сводилась к ускорению парсинга и проверки проксей.

Изначальный алгоритм был следующий:
1. Запрос к сервису, который предоставлял список прокси, который собирался в единый массив
2. Далее этот массив в цикле перебирался и с каждой отдельной проксей делался тестовый запрос для проверки ее работоспособности
3. Прокси успешно вернувшие тело ответа считались "работоспособными" и записывались в redis
Также для прокси выставлялись требования: таймаут для коннекта - 3сек, таймаут для запроса 15сек

Такой алгоритм достаточно успешно отрабатывал на прокси в количестве до 100, однако на большом количестве прокси (1000шт) он отрабатывал очень долго.
Это связанно с тем, суммарное время напрямую зависит от качества прокси. Среднее время ответа - 4сек, максимальное время - 15сек (падение по таймауту), что в результате приводит к затратам от 1 до 4 часов, что крайне много

## Формирование метрики
Для формирования метрики использовался срез из 100 проксей.

## Feedback-Loop
При помощи ```Benchmark.realtime``` замеряем время выполнения до и после оптимизации

### Находка №1
Очевидно, что последовательная проверка в одном потоке большого числа прокси крайне медленный процесс, поэтому было принято решение распараллелить проверку в несколько потоков.
В качестве обертки над Thread, использовался ```concurrent-ruby```

### Результаты
В среднем в однопоточном режиме:
* 100 прокси - 413 сек (около 6мин)
* 1000 прокси - 1.5 - 2 часа

Увеличив количество потоков до 10:
* 100 прокси - 44 сек
* 1000 - 7 - 10мин (в зависмости от качества исходных прокси)

По результатам оптимизации прирост фактически в 10 раз
